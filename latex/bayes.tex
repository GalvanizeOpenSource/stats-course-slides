\section{Bayes' Formula}

%
\begin{frame}
Remember our definition of conditional probability:

\begin{align*}
P(A \text{ and } B) &= P(A \mid B) P(B) \\
P(B \text{ and } A) &= P(B \mid A) P(A) \\
\end{align*}

Setting these equal to one another leads us to \textbf{Bayes' Formula}

$$ P(A \mid B) = \frac{ P(B \mid A) P(A) }{ P(B) } $$

\end{frame}
%

%
\begin{frame}

This is probably the most important simple formula in both probability and
statistics

$$ P(A \mid B) = \frac{ P(B \mid A) P(A) }{ P(B) } $$

\end{frame}
%

%
\begin{frame}

Let's study a classic thought experiment: the disease screening problem.

Suppose we have developed a test for a certain disease.
\begin{itemize}
\item Only 1 \% of people have the disease.
\item If a person has the disease, the test will be positive 99.9 \% of the
time.
\item If a person does not have the disease, the test will be negative 98 \% of
the time.
\end{itemize}

You get tested for the disease, and the test is positive.  \textbf{What is the
probability that you actually have the disease}?

\end{frame}
%

%
\begin{frame}
We are after the following conditional probability

$$ P(\text{Have Disease} \mid \text{Test is Positive)} $$

And we \textbf{know}

$$ P(\text{Have Disease}) = 0.01 $$
$$ P(\text{Test is Positive} \mid \text{Have Disease}) = 0.999 $$
$$ P(\text{Test is Positive} \mid \text{Do Not Have Disease}) = 0.02 $$
\end{frame}
%

%
\begin{frame}
Bayes' formula says:

\begin{align*}
P(\text{Have Disease}  & \mid \text{Test is Positive)} \\
%
& = \frac{  P(\text{Test is
Positive} \mid \text{Have Disease}) P(\text{Have Disease}) } { P(\text{Test is
Positive}) }
\end{align*}

We know all the things appearing in this formula except $ P(\text{Test is
Positive}) $.

\end{frame}
%

%
\begin{frame}

We can calculate the last piece by breaking things down

\begin{align*}
P(\text{Test} & \text{ is Positive}) \\
%
&= P(\text{Test is Positive and Have Disease}) \\
%
& \quad + P(\text{Test is Positive and Don't Have Disease}) \\
\end{align*}

\end{frame}
%

%
\begin{frame}

\begin{align*}
P(\text{Test} & \text{ is Positive and Have Disease}) \\
%
=& P(\text{Test is Positive} \mid \text{Have Disease}) P(\text{Have Disease}) \\
=& 0.999 \times 0.01 \\
\end{align*}

\end{frame}
%

%
\begin{frame}

\begin{align*}
P(\text{Test} & \text{ is Positive and Don't Have Disease}) \\
%
=& P(\text{Test is Positive} \mid \text{Don't Have Disease}) P(\text{Don't Have Disease}) \\
=& 0.02 \times 0.99 \\
\end{align*}

\end{frame}


\begin{frame}

We have all the pieces, so let's put them together

\begin{align*}
P(\text{Have Disease}  & \mid \text{Test is Positive}) = \\
%
&= \frac{  P(\text{Test is
Positive} \mid \text{Have Disease}) P(\text{Have Disease}) } { P(\text{Test is
Positive}) } \\
%
&= \frac{ 0.999 \times 0.01 } {  0.999 \times 0.01 +  0.02 \times 0.99 } \\
%
&= 0.34
\end{align*}

The probability we have the disease is only 34\%, \textbf{even knowing we
have received a positive test}.

\end{frame}
%

%
\begin{frame}

This kind of result is unintuitive to almost all humans, a mental bias called the
\textbf{base rate fallacy}.

\end{frame}
%

%
\begin{frame}

Pretty much \textbf{everyone's} intuition says that it should be much more
likely that the person does have the disease after a test comes back positive.
Pretty much \textbf{everyone} undervalues the prior information that

$$ P(\text{Have Disease}) = 0.01 $$

\textbf{It requires a lot of evidence to take an unlikely situation and make it
likely.}

\end{frame}
%

%
\begin{frame}

Here is a way to think about this.

Suppose there are 1000 total people (in the universe).  Then

\begin{itemize}
  \item $0.99 \times 1000 = 10$ actually have the disease.
  \item Of those, $10 \times 0.999 \approx 10$ will test poistive.
  \item Of the remaining that do not have the disease, about $990 \times 0.02 \approx 20$ will \textbf{also test positive}!
\end{itemize}

So if all you know is that you have received a positive test, you can only conclude that you are one of the thirty, of which only ten actually have the disease!

\end{frame}
%

%
\begin{frame}
There is some terminology that is often used to understand these relationships:

\begin{itemize}
\item $P(\text{Have Disease})$ is called the \textbf{prior probability}.  It is
what we know \textbf{before collecting evidence/data}.
\item $P(\text{Test is Positive} \mid \text{Have Disease})$ is called the
\textbf{likelihood}.  It is the \textbf{strength of the evidence/data we collected}.
\item $P(\text{Have Disease } \mid \text{Test is Positive})$ is called the
\textbf{posterior}.  It is what we know, \textbf{after collecting evidence/data}.
\end{itemize}

These ideas form the basis of \textbf{Bayesian statistics}.

\end{frame}
%

%
\begin{frame}
Suppose you now get a second test, which \textbf{also} comes out
\textbf{positive}. What is the posterior probability that you actually have the
disease?

\hfill

Suppose you get a second test, which comes back \textbf{negative}.  What is the
posterior probability that you actually have the disease.
\end{frame}
